
op _sqrt(input float32 x) -> output float32 {
    param float32 n = 0.5;
    temp float32 y = x ** n;
    return y;
}

op _expectation(
	input float32[N, C, H, W] X
) -> output float32[N] { 
	temp float32[N] E; 
	temp index[1:N] n; 
	temp index[1:C] c; 
	temp index[1:H] h; 
	temp index[1:W] w; 
	
	E[n] = sum[c, h, w](X[n, c, h, w]) / 1000; 
	return E;
}

op _variance(input float32[N, C, H, W] X) -> output float32[N] {
	temp float32[N] V;
	temp float32[N] E = _expectation(x)

	temp index[1:N] n;
	temp index[1:C] c;
	temp index[1:H] h;
	temp index[1:W] w;

	V[n] = sum[c, h, w]((X[n, c, h, w] - E[n]) **2)/1000;

}

# Layer Normalization
op LayerNormalization<T>( 
	input T[N, M] X, 
	param T[M] scale, # Scale parameter 
	param T[M] bias, # Bias parameter 
	param float32 eps 
) -> output T[N, M] { 
	temp T[N, M] Y; 
	temp T[N] E = _expectation(X); 
	temp T[N] V = _variance(X); 
	temp index[1:N] n; 
	temp index[1:M] m; 
	Y[n, m] = scale[m] * (X[n, m] - E[n]) / _sqrt(V[n] + eps) + bias[m];
	return Y; 
}

# Matrix Mult
op matmul<T>(
    input T[M, K] A,
    input T[K, N] B
) -> output T[M, N] {
    temp T[M, N] C;
    temp index[1:M] m;
    temp index[1:N] n;
    temp index[1:K] k;
    C[m, n] = sum[k](A[m, k] * B[k, n]);
    return C;
}



op Reshape<T>(
	input T[N, M] X,
	param tuple[int64, ...] new_shape
) -> output T[new_shpae] {
	temp T[new_shape] Y;
	#TODO: Implemet this logic not sure how to handle multi dim indexing
	
	return Y;
}

#transpose 
op Transpose<T>(
	input T[N, M] X
) -> output T[M, N] {
	temp T[M, N] Y;
	temp index[1:N] n;
	temp index[1:M] m;
	Y[m, n] = X[n, m];
	
	return Y;
}

op Div<T>(
	input T[N, M] A,
	input float32 B
) -> output T[N, M] {
	temp T[N, M] Y;
	
	temp index[1:N] n;
	temp index[1:M] m;
	Y[n, m] = A[n, m]/B
	
	return Y;
}

# Element wise Multiplication 
op Mult<T>(
	input T[N, M] A
	input float32 B
) -> output float32 {
	temp T[N, M] Y;
	
	temp index[1: N] n;
	temp index[1: M] m;
	
	Y[n, m] = A[n, m] * B[n, m];
	
	return Y;
}

op Add<T>(
	input T[N, M] A
	input float32 B
) -> output float32 {
	temp T[N, M] Y;
	
	temp index[1: N] n;
	temp index[1: M] m;
	
	Y[n, m] = A[n, m] + B[n, m];
	
	return Y;
}

op Softmax<T>( 
	input T[N, M] X 
) -> output T[N, M] { 
	temp T[N, M] Y; 
	temp index[1:N] n; 
	temp index[1:M] m; Y[n, m] = (_exp(X[n, m])) / sum[m](_exp(X[n, m]));
	return Y; 
}



op GELU<T>(input T[N, M] X) -> ouput T[N, M] {
	temp T[N, M] Y;
	temp index[1:N] n;
	temp index[1:M] m;

	# GELU approximation: 0.5 * x * (1 + tanh(sqrt(2/Ï€) * (x + 0.044715 * x^3))) from pytorch

	Y[n, m] = 0.5* X[n, m] * (1.0 + tanh(0.797884 * (X[n, m] + 0.044715 * X[n, m]**3)))
	return Y
	
}

# 1/1-p
op Dropout<T>(
	input T[N, M] X, 
	param float32 dropout_prob
) -> output T[N, M] {
	temp T[N, M] y;
	temp index[1:N] n;
	temp index[1:M] m;

	# place holder need a implementation 
	# to generate random values
	temp float32[N, M] mask;

	Y[n, m] = X[n, m] * mask[n,m] / (1.0 - dropout_prob);

	return Y;
}

# B = Batch size
# S = Sequence length 
# H = Hidden size
# simplified verision
op EmbeddingLookup<T>(
	input int32[B, S] input ids,
	param T[V, H] embedding_table
) -> output T[B, S, H] {
	temp T[B, S, H] embeddings;
	temp index[1:B] b;
	temp index[1:S] s;
	temp index[1:H] h;

	embeddings[b, s, h] = embedding_table[input_ids[b,s], h]
}

# might not be fully correct
op PositionEmbedding<T>(
	input T[B, S, H] input_embeddings;
	param T[S, H] position_embeddings;
	
) -> output T[B, S, H] {
	temp T[B, S, H] output_embeddings
	temp index[1:B] b;
	temp index[1:S] s;
	temp index[1:H] h;

	output_embeddings[b, s, h] = input_embeddings[b, s, h] + positon_embeddings[s,h];

	return output_embeddings;
}

# Attention mask creation only work for bert
op CreateAttentionMask( 
	input int32[B, S] input_mask 
) -> output float32[B, 1, 1, S] { 
	# instead of broadcasting directly assing values 
	temp float32[B, 1, 1, S] attention_mask; 
	temp index[1:B] b; 
	temp index[1:S] s; 
	attention_mask[b, 0, 0, s] = input_mask[b, s]; 
	
	return attention_mask; 
}

op PoolingLayer<T>(
	input T[B, S, H] sequence_output
) -> output T[B, H] {
	temp T[B, H] pooled_output;
	temp index[1:B] b;
	temp index[1:H] h;
	
	pooled_output[b, h] = sequence_ouputp[b, 0, h];

	return pooled_output;
}

# FeedForward procedure
proc FeedForward<T>(
    input T[B, S, H] input_tensor,
    param T[H, 4H] intermediate_weight,
    param T[4H, H] output_weight,
    param T[4H] intermediate_bias,
    param T[H] output_bias,
    output T[B, S, H] output_tensor
) {
    temp T[B, S, 4H] intermediate_output;
    temp T[B, S, H] layer_output;
    temp index[1:B] b;
    temp index[1:S] s;
    temp index[1:4H] h4;
    temp index[1:H] h;

    # linear transformation
    intermediate_output[b, s, h4] = sum[h](input_tensor[b, s, h] * intermediate_weight[h, h4]) + intermediate_bias[h4];

    # GELU activation
    intermediate_output = GELU(intermediate_output);

    # Second linear transformation
    layer_output[b, s, h] = sum[h4](intermediate_output[b, s, h4] * output_weight[h4, h]) + output_bias[h];

    output_tensor = layer_output;
}


proc SelfAttention<T>(
    input T[B, S, H] input_tensor,
    param T[H, H] query_weight, key_weight, value_weight,
    param T[H] attention_output_bias,
    param float32 layer_norm_eps,
    input float32[B, 1, 1, S] attention_mask,
    param float32 attention_probs_dropout_prob,
    output T[B, S, H] attention_output
) {
    temp T[B, S, H] query_layer, key_layer, value_layer;
    temp T[B, S, S] attention_scores;
    temp T[B, S, S] attention_probs;
    temp float32 attention_head_size = float32(H);

    # Linear transformations for Q, K, V
    query_layer = matmul(input_tensor, query_weight);
    key_layer = matmul(input_tensor, key_weight);
    value_layer = matmul(input_tensor, value_weight);

    # attention scores
    attention_scores = matmul(query_layer, Transpose(key_layer));
    attention_scores = Div(attention_scores, _sqrt(attention_head_size));

    # attention mask
    attention_scores = Add(attention_scores, (1.0 - attention_mask) * -10000.0);

    # softmax to get attention prob
    attention_probs = Softmax(attention_scores);

    # attention dropout
    attention_probs = Dropout(attention_probs, attention_probs_dropout_prob);

    # weighted sum of values
    attention_output = matmul(attention_probs, value_layer);

    # output bias
    attention_output = Add(attention_output, attention_output_bias);
}

proc BERTLayer<T>(
    input T[B, S, H] input_tensor,
    param T[H, H] query_weight, key_weight, value_weight,
    param T[H] attention_output_bias,
    param T[H, 4H] intermediate_weight,
    param T[4H, H] output_weight,
    param T[4H] intermediate_bias,
    param T[H] output_bias,
    param T[H] attention_layer_norm_scale, attention_layer_norm_bias,
    param T[H] output_layer_norm_scale, output_layer_norm_bias,
    param float32 layer_norm_eps,
    input float32[B, 1, 1, S] attention_mask,
    param float32 hidden_dropout_prob,
    param float32 attention_probs_dropout_prob,
    output T[B, S, H] layer_output
) {
    temp T[B, S, H] attention_output;
    temp T[B, S, H] attention_output_dropout;
    temp T[B, S, H] attention_output_layer_norm;
    temp T[B, S, H] intermediate_output;
    temp T[B, S, H] layer_output_dropout;

    # Self-Attention with mask and dropout
    SelfAttention(input_tensor, query_weight, key_weight, value_weight, attention_output_bias, layer_norm_eps, attention_mask, attention_probs_dropout_prob, attention_output);
    attention_output_dropout = Dropout(attention_output, hidden_dropout_prob);
    attention_output_layer_norm = LayerNormalization(Add(attention_output_dropout, input_tensor), attention_layer_norm_scale, attention_layer_norm_bias, layer_norm_eps);

    # Feed-forward network
    FeedForward(attention_output_layer_norm, intermediate_weight, output_weight, intermediate_bias, output_bias, intermediate_output);
    layer_output_dropout = Dropout(intermediate_output, hidden_dropout_prob);
    layer_output = LayerNormalization(Add(layer_output_dropout, attention_output_layer_norm), output_layer_norm_scale, output_layer_norm_bias, layer_norm_eps);
}

proc main(
    input int32[B, S] input_ids,
    input int32[B, S] input_mask,
    input int32[B, S] token_type_ids,
    param float32[V, H] word_embeddings,
    param float32[S, H] position_embeddings,
    param float32[2, H] token_type_embeddings,
    param float32[H] embedding_layer_norm_scale,
    param float32[H] embedding_layer_norm_bias,
    param float32[L, H, H] query_weight,
    param float32[L, H, H] key_weight,
    param float32[L, H, H] value_weight,
    param float32[L, H] attention_output_bias,
    param float32[L, H, 4H] intermediate_weight,
    param float32[L, 4H, H] output_weight,
    param float32[L, 4H] intermediate_bias,
    param float32[L, H] output_bias,
    param float32[L, H] attention_layer_norm_scale,
    param float32[L, H] attention_layer_norm_bias,
    param float32[L, H] output_layer_norm_scale,
    param float32[L, H] output_layer_norm_bias,
    param int32 num_hidden_layers,
    param float32 hidden_dropout_prob,
    param float32 attention_probs_dropout_prob,
    param float32 layer_norm_eps,
    output float32[B, S, H] sequence_output,
    output float32[B, H] pooled_output
) {
    # Embedding layer
    temp float32[B, S, H] embedding_output = EmbeddingLookup(input_ids, word_embeddings);
    embedding_output = PositionEmbedding(embedding_output, position_embeddings);
    
    # Add token type embeddings
    temp float32[B, S, H] token_type_embeddings_output = EmbeddingLookup(token_type_ids, token_type_embeddings);
    embedding_output = Add(embedding_output, token_type_embeddings_output);
    
    # Layer normalization and dropout for embeddings
    embedding_output = LayerNormalization(embedding_output, embedding_layer_norm_scale, embedding_layer_norm_bias, layer_norm_eps);
    embedding_output = Dropout(embedding_output, hidden_dropout_prob);
    
    # Create attention mask
    temp float32[B, 1, 1, S] attention_mask = CreateAttentionMask(input_mask);
    
    # Transformer layers
    temp float32[B, S, H] layer_output = embedding_output;
    temp index[1:L] l;
    
    # First layer uses embedding_output as input
    layer_output = BERTLayer(
        embedding_output,
        query_weight[0], key_weight[0], value_weight[0],
        attention_output_bias[0],
        intermediate_weight[0],
        output_weight[0],
        intermediate_bias[0],
        output_bias[0],
        attention_layer_norm_scale[0], attention_layer_norm_bias[0],
        output_layer_norm_scale[0], output_layer_norm_bias[0],
        layer_norm_eps,
        attention_mask,
        hidden_dropout_prob,
        attention_probs_dropout_prob
    );

    # Subsequent layers use the output of the previous layer as input
    layer_output = BERTLayer(
        layer_output,
        query_weight[l], key_weight[l], value_weight[l],
        attention_output_bias[l],
        intermediate_weight[l],
        output_weight[l],
        intermediate_bias[l],
        output_bias[l],
        attention_layer_norm_scale[l], attention_layer_norm_bias[l],
        output_layer_norm_scale[l], output_layer_norm_bias[l],
        layer_norm_eps,
        attention_mask,
        hidden_dropout_prob,
        attention_probs_dropout_prob
    );
    
    sequence_output = layer_output;
    pooled_output = PoolingLayer(sequence_output);
}